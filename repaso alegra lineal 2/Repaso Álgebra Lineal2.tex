\documentclass{beamer}
\usetheme{ALUF}

\usepackage[utf8]{inputenc}
% \usepackage{palatino}
% \usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[expert]{mathdesign}
\usepackage[protrusion=true,expansion=true,tracking=true,kerning=true]{microtype}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
%% Use any fonts you like.
% \usepackage{libertine}

\title{Repaso de Álgebra lineal 2}
\subtitle{Reconocimiento de patrones}
\author{Gamaliel Moreno}
\date{Enero-Julio 2021}
%\institute{\url{gamalielmch@uaz.edu.mx}}

\institute{\url{gamalielmch@uaz.edu.mx}\\\url{http://pds.uaz.edu.mx/}}

\begin{document}

\begin{frame}[plain,t]
\titlepage
\end{frame}

\begin{frame}% [plain,t]
	\frametitle{Contenido}
\tableofcontents
\end{frame}

%=============================================================================================

\section{Otras operaciones}

\begin{frame}
\frametitle{Inversa de una matriz}
 \begin{itemize}
 \item Una matriz cuadrada $\boldsymbol{A}$ es invertible (o no singular) si existe otra matriz $\boldsymbol{A}^{-1}$ tal que 
 \begin{equation*}
\boldsymbol{AA}^{-1} = \boldsymbol{A}^{-1}\boldsymbol{A}= \boldsymbol{I}
\end{equation*}
\item Caso contrario, la matriz se dice singular (o no invertible)
\item Una matriz es ortogonal si se cumple $\boldsymbol{A}^{-1}= \boldsymbol{A}^{T}$, es decir, si sus vectores (fila y columna) son ortonormales entre sí.
\item Una matriz ortogonal no cambia la norma $l_2$ de un vector $\vert \vert \boldsymbol{Ax}\vert \vert_{2}= \vert \vert \boldsymbol{x}\vert \vert_{2}$  
 \end{itemize}

\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Determinantes de una matriz 1}

\begin{itemize}
\item La determinante de una matriz cuadrada es un valor escalar denotado como $\vert \boldsymbol{A}\vert$ o $ det\boldsymbol{ A}$
\item Interpretación: Sea $\boldsymbol{\alpha}$ un vector con $0\leq \alpha_{i} \leq 1$. Todos los vectores que cumplen esta condición conforman un hipercubo en el espacio n dimensional. La determinante indica el volumen de ese hipercubo transformado por la matriz $\boldsymbol{A}$ 
\item La determinant de una matriz $2x2$ se calcula con 
\begin{equation*}
\vert \boldsymbol{A} \vert = \left| \begin{array}{cc} a_{11} &  a_{12} \\ a_{21} &  a_{22}  \end{array} \right|= a_{11}a_{22} -   a_{12}a_{21}
\end{equation*}
\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Determinantes de una matriz 1}

\begin{itemize}

\item Para una matriz de mayor orden la determinante es 
\begin{equation*}
\vert \boldsymbol{A} \vert= \displaystyle \sum_{j=1}^{n}{a_{fj} \vert \boldsymbol{A}_{fj}\vert  }
\end{equation*}
donde $\boldsymbol{A}_{fj}$ es la matriz adjunta de $a_{fj}$ dada por $(-1)^{f+j}\boldsymbol{M}_{fj}$ y $\boldsymbol{M}_{fj}$ es el menor complementario de $a_{fj}$, es decir, una matriz $(n-1)\times(n-1)$ obtenida eliminando la fila $f$ y la columna $j$ de la matriz $\boldsymbol{A}$ 
\end{itemize}

\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Propiedades de la determinante}
Sea $\boldsymbol{A}$ una matriz cuadrada de $n  \times n$ 
\begin{itemize}
\item $\vert s \boldsymbol{A} \vert= s^n \vert \boldsymbol{A} \vert$
\item $\vert \boldsymbol{I} \vert =1$ 
\item Distributividad: $\vert  \boldsymbol{AB} \vert = \vert  \boldsymbol{A} \vert \vert  \boldsymbol{B} \vert$ 
\item $\vert \boldsymbol{I} \vert = 1 = \vert \boldsymbol{A} \boldsymbol{A}^{-1}\vert =\vert \boldsymbol{A} \vert\vert \boldsymbol{A}^{-1}\vert \Rightarrow \vert \boldsymbol{A}^{-1}\vert= 1/ \vert \boldsymbol{A}\vert$
\item $\vert \boldsymbol{A} \vert = \vert \boldsymbol{A}^{T}\vert$
\item $\vert \boldsymbol{A} \vert =0 $ sí y sólo sí $\boldsymbol{A}$ es singular
\end{itemize}


\end{frame}
%%
%=============================================================================================
\begin{frame}
\frametitle{Normas}
\begin{itemize}
\item Informalmente, la norma de $\Vert \boldsymbol{x}\Vert$ es una medida de la longitud del vector $\boldsymbol{x} \in \mathbb{R}^n$
\item Formalmente, la norma es un operador $\Vert \cdot \Vert :\mathbb{R}^n \rightarrow \mathbb{R}$ que satisface 4 propiedades: 
\begin{enumerate}
\item $\forall \boldsymbol{x} \in \mathbb{R}^n, \Vert \boldsymbol{x}\Vert \geq 0$ (no negatividad)
\item  $\Vert \boldsymbol{x}\Vert=0 \Leftrightarrow \boldsymbol{x}=0$ (definitud)
\item $\forall \boldsymbol{x} \in \mathbb{R}^n, a\in \mathbb{R},\Vert a\boldsymbol{x}\Vert = \vert a \vert \Vert \boldsymbol{x}\Vert $ (Homogeneidad)
\item $\forall \boldsymbol{x} \in \mathbb{R}^n,  \Vert \boldsymbol{x+y}\Vert \leq \Vert \boldsymbol{x}\Vert + \Vert \boldsymbol{y}\Vert$ (desigualdad de Minkowski)  
\end{enumerate}

\end{itemize}


\end{frame}
%
%=============================================================================================
\begin{frame}
\frametitle{Ejemplo de normas}
\begin{itemize}
\item Normas $ \ell_{p}$ de Minkowski o normas $p$ para vectores $\boldsymbol{x} \ in \mathbb{R}^n :$
\begin{equation*}
\Vert \boldsymbol{x}\Vert_{p}= \left( \sum_{i=1}^{n} \vert x_{i} \vert^{p} \right)^{1/p} 
\end{equation*}
casos particulares 
\item Formalmente, la norma es un operador $\Vert \cdot \Vert :\mathbb{R}^n \rightarrow \mathbb{R}$ que satisface 4 propiedades: 
\begin{itemize}

\item Norma $\ell_2$ o euclidiana: $\Vert \boldsymbol{x} \Vert_{2}= \sqrt{\boldsymbol{x}^T\boldsymbol{x}}= \sqrt{\sum_{i=1}^{n}x_{i}^2}$
\item Norma $\ell_1$ o de bloques de ciudad:$\Vert \boldsymbol{x} \Vert_{1}= \sum_{i=2}^{n} \vert x_{i}\vert$ 
\item Norma $\ell_\infty : \Vert \boldsymbol{x}\Vert_{\infty}= max \vert x_{i} \vert$   
\end{itemize}
\item Norma de Frobenius para matrices $\boldsymbol{A} \in \mathbb{R}^{m\times n}$
\begin{equation*}
\Vert \boldsymbol{A} \Vert_{F}= \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}^{2}}= \sqrt{tr(\boldsymbol{A}^T\boldsymbol{A})}
\end{equation*}
\end{itemize}


\end{frame}
%
%=============================================================================================
\section{Conceptos avanzados}
\subsection{Independencia Lineal}
\begin{frame}
\frametitle{Independencia lineal}
\begin{itemize}
\item El conjunto $ \left\lbrace  \boldsymbol{x_1,x_2, \ldots, x_m } \right\rbrace \subset \mathbb{R}$ es linealmente independiente si ningún vector puede expresarse como combinación lineal de los otros vectores. Este es el caso sólo si 
\begin{equation*}
\sum_{i=1}^{m}\alpha_{i}\boldsymbol{x_{i}}= 0 \Leftrightarrow \alpha_{1}=\alpha_2=\ldots= \alpha_{m}=0
\end{equation*}   
\item Caso contrario se dice que los vectores son linealmente dependientes. 
\begin{equation*}
\boldsymbol{x}_j=  \sum_{i=1,\ldots, m, i\neq j} \alpha_{i}\boldsymbol{x}_{i}
\end{equation*}
\end{itemize}
\end{frame}
%%%=============================================================================================

\begin{frame}
\frametitle{Bases}
\begin{itemize}
\item Si $\mathcal{B}$ es un conjunto generador de un espacio lineal $\mathbb{V}$, $\mathcal{B}$ es una base de $\mathbb{V}$ si y sólo si todos sus vectores son linealmente independientes.
\item El número de vectores en la base $n= \vert \mathcal{B}\vert$ es igual a la dimensión de $\mathbb{V}$ 
\item Un conjunto generador de $\mathbb{V}$ requiere al menos $n$ vectores
\item Un conjunto generador linealmente independiente puede tener a lo sumo $n$ vectores 
\end{itemize}
\end{frame}
%%%=============================================================================================
\subsection{Sistemas de ecuaciones y espacios de la matriz}
\begin{frame}
\frametitle{Sistemas de ecuaciones en notación matricial 1}
Todo sistema de ecuaciones lineales se puede expresa de la forma 
\begin{equation*}
\boldsymbol{A}\boldsymbol{x}= \boldsymbol{b}
\end{equation*}
que representa a 
\begin{equation*}
 \begin{bmatrix} a_{11} &  a_{12} & \dots & a_{1n}\\ a_{21} &  a_{22} & \dots & a_{2n} \\ \vdots &  \vdots & \ddots & \vdots \\ a_{n1} &  a_{n2} & \dots & a_{nn} \end{bmatrix}  \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} =\begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix}
\end{equation*}
o en forma tradicional 
\begin{equation*}
\begin{array}{lcl} a_{11}x_{1} +  a_{12}x_{2} +  \cdots + a_{1n}x_{n} & = & b_1 \\ a_{21}x_{1} +  a_{22}x_{2} +  \cdots + a_{2n}x_{n} & = & b_2 \\ \hspace{1em} \vdots \hspace{0.8em} +  \hspace{1em}  \vdots \hspace{1em} + \hspace{0.5em}\vdots \hspace{0.5em}+ \hspace{1em}\vdots \hspace{1em}& =&  \vdots\\  a_{n1}x_{1} +  a_{n2}x_{2} +  \cdots + a_{nn}x_{n} & = & b_n \end{array}
\end{equation*}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Sistemas de ecuaciones en notación matricial 2}
Si $ \boldsymbol{b}$ está en el alcance columna de $\boldsymbol{A}$ entonces el sistema tiene solución:
\begin{equation*}
\boldsymbol{A}\boldsymbol{x}= \boldsymbol{b}
\end{equation*}

El rango columna de $\boldsymbol{A}$ (rank) es el máximo número de columnas linealmente independientes, y es igual a la dimensión del alcance columna (range) de $\boldsymbol{A}$ 

El espacio nulo de $\boldsymbol{A}$ es el conjunto de vectores $\boldsymbol{z}$ tales que $\boldsymbol{Az}=0$. Su dimensión se llama nulidad. 

La nulidad más el rango es igual al número de filas $\boldsymbol{A}$- 

De forma dual, para el sistema $\boldsymbol{x}^T \boldsymbol{A}$ se define el espacio fila como la combinación lineal de las filas de $\boldsymbol{A}$, que tiene su alcance y rango fila.

Para $\boldsymbol{A}\in \mathbb{R}^{m\times c}$, se cumple que el rango columna y el rango fila son siempre iguales, y se denota como $rg\boldsymbol{A}$. 

\end{frame}
%%%=============================================================================================

%=============================================================================================
\begin{frame}
\frametitle{Propiedades del rango}
\begin{itemize}
\item Para $\boldsymbol{A} \in \mathbb{R}^{m\times n}, rg(\boldsymbol{A}) \leq min(m,n)$. Si $rg(\boldsymbol{A}) = min (m,n)$, entonces se dice que $\boldsymbol{A}$ tiene rango completo. 
\item Solo las matrices de rango completo son invertibles 
\item $rg(\boldsymbol{A})=rg(\boldsymbol{A}^T)$
\item $rg(\boldsymbol{AB}) \leq min (rg(\boldsymbol{A}),rg(\boldsymbol{B}))$
\item $rg(\boldsymbol{A+B}) \leq rg(\boldsymbol{A}) + rg(\boldsymbol{B})$
\end{itemize}

\end{frame}
%%%=============================================================================================

\subsection{Forma cuadrática}
\begin{frame}
\frametitle{Forma cuadrática y matrices (semi) definida positivas}


\begin{itemize}
\item La forma cuadrática es la expresión escalar $\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}$, calculada con: 
\begin{equation*}
\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}= \sum_{i=1}^{n} x_{i} (\boldsymbol{A}\boldsymbol{x})_{i} = \sum_{i=1}^{n} x_{i} \left( \sum_{j=1}^{n} a_{ij}x_{j}\right)=  \sum_{i=1}^{n}\sum_{j=1}^{n} a_{ij}x_{i}x_{j}
\end{equation*} 
\item como la forma cuadrática es escalar y $s=s^T$ entonces 
\begin{equation*}
\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}= (\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x})^T= \boldsymbol{x}^T\boldsymbol{A}^T\boldsymbol{x}
\end{equation*}

\item como los tres términos anteriores son iguales, entonces
\begin{equation*}
\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}=\frac{1}{2} (\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}+ \boldsymbol{x}^T\boldsymbol{A}^T\boldsymbol{x})= \boldsymbol{x}^T \left( \frac{1}{2} (\boldsymbol{A}+\boldsymbol{A}^T)\right)  \boldsymbol{x} 
\end{equation*}
es decir, el valor solo depende de la componente simétrica de $\boldsymbol{A}$.
\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Positividad o negatividad (semi)definida}
La matriz simétrica $\boldsymbol{A} \in \mathbb{S}^n$ es 
\begin{itemize}
\item Positiva definida si $ \forall \boldsymbol{x} \in \mathbb{R}^n  \setminus 0 \quad \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}>0$. \\
Notación: $\boldsymbol{A}\succ 0 (o \boldsymbol{A}>0)$.\\
Todas las matrices positivas definidas: $\mathbb{S}^{n}_{++}$ 
\item Positiva semidefinida $ \forall \boldsymbol{x} \in \mathbb{R}^n  \quad \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}\geq 0$\\
Notación: $\boldsymbol{A}\succeq 0 (o \boldsymbol{A}\geq 0)$.\\
Todas las matrices positivas definidas: $\mathbb{S}^{n}_{+}$ 
\item Negativa definida si $ \forall \boldsymbol{x} \in \mathbb{R}^n \setminus 0 \quad \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}<0$.\\
Notación: $\boldsymbol{A}\prec 0 (o \boldsymbol{A}<0)$.\\
Todas las matrices positivas definidas: $\mathbb{S}^{n}_{--}$ 
\item Negativa semidefinida si $ \forall \boldsymbol{x} \in \mathbb{R}^n  \quad \boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}\geq 0$.\\
Notación: $\boldsymbol{A}\preceq 0 (o \boldsymbol{A}\leq 0)$.\\
Todas las matrices positivas definidas: $\mathbb{S}^{n}_{-}$ 
\item Indefinida en cualquier otro caso

\end{itemize}


\end{frame}
%
%%%=============================================================================================
\begin{frame}
\frametitle{Características de matrices (semi)definida}

\begin{itemize}
\item Si $ \boldsymbol{A} \in \mathbb{S}^{n}_{++}$ entonces, $ -\boldsymbol{A} \in  \mathbb{S}^{n}_{--}$ 
\item Si $ \boldsymbol{A} \in \mathbb{S}^{n}_{+}$ entonces, $ -\boldsymbol{A} \in  \mathbb{S}^{n}_{-}$ 
\item Todas las matrices positivas (o negativas) definidas sin de rango completo. 

\item Sea $\boldsymbol{A}\in \mathbb{R}^{m\times n}$ una matriz cualquiera 
\begin{itemize}
\item  $\boldsymbol{A}^{T} \boldsymbol{A}$ se denomina matriz Gram
\item  $\boldsymbol{A}^{T} \boldsymbol{A}$ es siempre positiva semidefinida
\item Si $m\geq n$ y $\boldsymbol{A}$ es de rango completo entonces $\boldsymbol{A}^{T} \boldsymbol{A}$ es positiva definida 
\end{itemize}
\end{itemize}

\end{frame}%=============================================================================================

\subsection{Eigensistemas}
\begin{frame}
\frametitle{Eigenvalores y eigenvectores}

\begin{itemize}
\item También llamados vectores propios, autovectores o vectores característicos
\item Dada una matriz cuadrada $\boldsymbol{A} \in \mathbb{R}^{n \times n}$, se dice que $ \lambda \in \mathbb{C}$ es un eigenvalor de $\boldsymbol{A}$ y $ \boldsymbol{x} \in  \mathbb{C}^n$ es su correspondiente eigenvector si 

\begin{equation*}
\boldsymbol{A}\boldsymbol{x}= \lambda\boldsymbol{x}, \quad \boldsymbol{x} \neq 0
\end{equation*}

\item Interpretación: transformación de eigenvector $\boldsymbol{x}$ con $\boldsymbol{A}$ solo cambia su magnitud con un factor $\lambda$
\item Cualquier escalamiento de un eigenvector es también eigenvector
\begin{equation*}
\boldsymbol{A}(c \boldsymbol{x})= c(\boldsymbol{A}\boldsymbol{x})= c\lambda \boldsymbol{x}=  \lambda(c \boldsymbol{x})
\end{equation*}
se usan eigenvectores normalizados ($\Vert \boldsymbol{x} \Vert_{2}=1$)
\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Eigenvalores}
\begin{itemize}
\item Multiplicando por matriz identidad no modifica nada
\begin{equation*}
\begin{array}{rcl}
\boldsymbol{A}\boldsymbol{x}&=& \lambda  \boldsymbol{I}\boldsymbol{x}\\
 (\boldsymbol{A}- \lambda \boldsymbol{I}) \boldsymbol{x}&=&0
\end{array}
\end{equation*}
que tiene como solución trivial $\boldsymbol{x}=0$
\item Otras soluciones con $\boldsymbol{x}\neq 0$ posibles solo si $ \boldsymbol{A}- \lambda \boldsymbol{I}$ es singular (tiene nulidad mayor que cero), por lo que 
\begin{equation*}
det \vert \boldsymbol{A}- \lambda \boldsymbol{I}\vert = 0
\end{equation*}
\end{itemize}

\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Eigenvalores}
Puesto que 
  
\begin{equation*}
\boldsymbol{A}- \lambda \boldsymbol{I} = \begin{bmatrix} a_{11} & a_{12}  & \cdots & a_{1n}  \\   a_{21} & a_{22 } & \cdots & a_{2n}   \\ \vdots & \vdots   & \ddots & \vdots  \\   a_{n1} & a_{n2}  & \cdots & a_{nn}  \end{bmatrix}- \begin{bmatrix} \lambda & 0  & \cdots & 0  \\   0 & \lambda  & \cdots & 0   \\ \vdots & \vdots   & \ddots & \lambda \\   0 & 0  & \cdots & 0  \end{bmatrix}= 
\end{equation*}

\begin{equation*}
\begin{bmatrix} a_{11}-\lambda & a_{12}  & \cdots & a_{1n}  \\   a_{21} & a_{22}-\lambda  & \cdots & a_{2n }  \\ \vdots & \vdots   & \ddots & \vdots  \\   a_{n1} & a_{n2}  & \cdots & a_{nn} -\lambda  \end{bmatrix}
\end{equation*}




\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Eigenvalores}
\begin{itemize}
\item La ecuación $det\vert \boldsymbol{A}- \lambda \boldsymbol{I}=0$ es entonces

  

\begin{equation*}
det \left|
\begin{array}{cccc} a_{11}-\lambda & a_{12}  & \cdots & a_{1n}  \\   a_{21} & a_{22}-\lambda  & \cdots & a_{2n }  \\ \vdots & \vdots   & \ddots & \vdots  \\   a_{n1} & a_{n2}  & \cdots & a_{nn} -\lambda  
\end{array} \right|=0
\end{equation*}
que produce un polinomio de orden $n$ en términos de $\lambda$.
\item Existen por tanto $n$ soluciones (posiblemente complejas)
\item Problema: ecuación mal condicionado (se usan otros métodos de cálculode $\lambda$)
\end{itemize}


\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Eigenvalores}
\begin{itemize}
\item Para encontrar uno de los eigenvalores $\lambda_{i}$ se plantea un sistema de ecuaciones para encontrar el eigenvector correspondiente 

\begin{equation*}
( \boldsymbol{A}- \lambda_{i} \boldsymbol{I})\boldsymbol{x}_{i}=0
\end{equation*}  


\item Nótese que soluciones $\lambda$ pueden ser complejas o tener multiplicidad mayor a 1. Esto implica que puede 
\begin{itemize}
\item No existan eigenvectores
\item Existen menos eigenvectores que la dimensión del espacio 
\item Nunca van a existir más eigenvectores que la dimensión del espacio 
\end{itemize}


\end{itemize}

\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Propiedades de eigenvalores y eigenvectores}
\begin{itemize}
\item $tr \boldsymbol{A} = \sum_{i}^{n}{\lambda_{i}}$
\item $ \vert \boldsymbol{A} \vert \prod_{i}^{n} \lambda_{i}$
\item $rg \boldsymbol{A} = \vert \lbrace \lambda \vert \lambda \neq 0 \rbrace \vert$
\item Si $\exists \boldsymbol{A}^{-1} \Longrightarrow \boldsymbol{A}^{-1}\boldsymbol{x}_{i}= (1/\lambda_{i})\boldsymbol{x}_i$ 
\item Los eigenvalores de $\boldsymbol{D}= diag(d_1,\ldots,d_n)$ son $\lambda_i=d_i$
\end{itemize}



\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Expresión simultánea}
\begin{itemize}
\item Todos los eigenvectores se expresan simultáneament con 
\begin{equation*}
\boldsymbol{A}\boldsymbol{X}=\boldsymbol{X}\boldsymbol{\Lambda}
\end{equation*}
con 
\begin{equation*}
\boldsymbol{X} \in \mathbb{R}^{n\times n}=  \begin{bmatrix}	\tcboxmath[colback=blue!25!white,colframe=blue,top=20pt,left=0pt,right=0pt,bottom=20pt]{\boldsymbol{x_1}} & {\cdots} & \tcboxmath[colback=blue!25!white,colframe=blue,top=20pt,left=0pt,right=0pt,bottom=20pt]{\boldsymbol{x_n}}  \end{bmatrix}
\end{equation*}
\begin{equation*}
\boldsymbol{\Lambda}=diag(\lambda_{1}, \ldots, \lambda{n})= \begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix}
\end{equation*}
\end{itemize}

\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Diagonalización}
\begin{itemize}
\item Si los eigenvectores de $\boldsymbol{A}$ son linealmente independientes, entonces $\boldsymbol{X}$ es invertible. En ese caso

\begin{equation*}
\begin{array}{rcl}
\boldsymbol{A}\boldsymbol{X}&=&\boldsymbol{X}\boldsymbol{\Lambda}\\
\boldsymbol{A}\boldsymbol{X}\boldsymbol{X}^{-1}&=&\boldsymbol{X}\boldsymbol{\Lambda}\boldsymbol{X}^{-1}\\
\boldsymbol{A}&=&\boldsymbol{X}\boldsymbol{\Lambda}\boldsymbol{X}^{-1}
\end{array}
\end{equation*}

\item En ese caso, la metriz se denomina diagonalizable 
\begin{equation*}
\begin{array}{rcl}
\boldsymbol{A}\boldsymbol{X}&=&\boldsymbol{X}\boldsymbol{\Lambda}\\
\boldsymbol{X}^{-1}\boldsymbol{A}\boldsymbol{X}&=&\boldsymbol{X}^{-1}\boldsymbol{X}\boldsymbol{\Lambda}\\
\boldsymbol{X}^{-1}\boldsymbol{A}\boldsymbol{X}&=&\boldsymbol{\Lambda}
\end{array}
\end{equation*}
\end{itemize}

\end{frame}
%=============================================================================================
%=============================================================================================
\end{document}
