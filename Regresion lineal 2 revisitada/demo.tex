\documentclass[10pt]{beamer}


\usetheme[progressbar=frametitle]{metropolis}
\usecolortheme{wolf}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays


\title{Regresión y optimización}
\subtitle{Reconocimiento de patrones}
\author{Gamaliel Moreno}
\institute{UAZ-MCPI}
\date{Enero-julio 2021}
\titlegraphic{%
    \includegraphics[width=.5\textwidth]{ncstate_red_letter_logo.png}\hfill
}

\makeatletter
\setbeamertemplate{title page}{
  \begin{minipage}[b][\paperheight]{\textwidth}
    \vfill%
    \ifx\inserttitle\@empty\else\usebeamertemplate*{title}\fi
    \ifx\insertsubtitle\@empty\else\usebeamertemplate*{subtitle}\fi
    \usebeamertemplate*{title separator}
    \ifx\beamer@shortauthor\@empty\else\usebeamertemplate*{author}\fi
    \ifx\insertdate\@empty\else\usebeamertemplate*{date}\fi
    \ifx\insertinstitute\@empty\else\usebeamertemplate*{institute}\fi
    \vfill
    \ifx\inserttitlegraphic\@empty\else\inserttitlegraphic\fi
    \vspace*{1cm}
  \end{minipage}
}
\makeatother

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \titlepage
\end{frame} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents%[hideallsubsections]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Regresión lineal}
\subsection{Interpretación probabilística}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Recordando}

\begin{itemize}
\item $(\boldsymbol{x}^{(i)}, y^{(i)})$: i-ésimo dato de entrenamiento 
\item $h_{\theta}(\boldsymbol{x}^{(i)}))$: predicción de hipótesis $h_{\theta}$ para dato de entrada $\boldsymbol{x}^{(i)})$ 
\begin{equation*}
h_{\theta}(\boldsymbol{x}) = \sum_{j=0}^{n}{\theta_{j}x_{j}}= \boldsymbol{\theta}^{T}\boldsymbol{x}
\end{equation*}
donde asumimos $x_{0}=1$. El número de características es n
\item Función cuadrática de costo:
\begin{equation*}
J(\boldsymbol{\theta})= \frac{1}{2} \sum_{i=1}^{m}{h_{\theta}(\boldsymbol{x}^{(i)}- y^{(i)})^2}
\end{equation*}
con $m$ el número de datos en el conjunto de entrenamiento 
\item Analíticamente: $\boldsymbol{\theta}^{*}= \argmin\limits_{\boldsymbol{\theta}} J(\boldsymbol{\theta})= (\boldsymbol{X}^T\boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{y}$
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Función cuadrática de costo}
\begin{itemize}
\item ¿Por qué usamos una función cuadrática de costo?
\begin{equation*}
J(\boldsymbol{\theta})= \frac{1}{2} \sum_{i=1}^{m} {h_{\boldsymbol{\theta}}(\boldsymbol{x}^{(i)})- y^{(i)})^2}
\end{equation*}
como $m$ el número de datos en el conjunto de entrenamiento.
\item Pudimos usar otras cosas: valor absoluto , o potencia de 4...
\item Vamos a analizar el problema de regresión desde una perspectiva probabilística, como posible argumentación para esto 
\end{itemize}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Regresión lineal}

\begin{itemize}
\item Supongamos que la salida y las entradas siguen el modelo de regresión 
\begin{equation*}
y^{(i)}= {\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)} \epsilon^{(i)} \Rightarrow   \epsilon^{(i)}= y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)} 
\end{equation*}
\item El término de error $\epsilon^{(i)}$ captura
\begin{itemize}
\item Aspectos no modelos 
\item Ruido aleatorio
\end{itemize}

\item Supondremos que $\epsilon^{(i)}$ son independientes e idénticamente distribuidos (i.i.d.)

\item Como proceso es complejo, supondremos que $\epsilon^{(i)}\sim \mathcal{N}(0,\sigma^2)$ y por tanto la PDF de $\epsilon^{(i)}$ es 

\begin{equation*}
p(\epsilon^{(i)})= \frac{1}{\sqrt{2\pi \sigma}} exp \left( -\frac{(\epsilon^{(i)})^2}{2\sigma^2} \right) 
\end{equation*}
\item Introduciendo el modelo $\epsilon^{(i)} = y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)} $ derivamos 
\begin{equation*}
p(y^{(i)}\vert\boldsymbol{x}^{(i)}; \boldsymbol{\theta} )=   \frac{1}{\sqrt{2\pi \sigma}} exp \left( -\frac{(y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)})^2}{2\sigma^2} \right) 
\end{equation*}
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Regresión lineal}

\begin{itemize}
\item Nótese que $\boldsymbol{\theta}$ está fuera de la condición, pues no es una variable aleatoria. La asumimos como dada.

\item La distribución $ y^{(i)}\vert\boldsymbol{x}^{(i)}; \boldsymbol{\theta}  \sim \mathcal{N} ({\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)}, \sigma^2)$

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Verosimilitud}

\begin{itemize}
\item Con la matriz de diseño $\boldsymbol{X}$ y los parámetros de $\boldsymbol{\theta} $, la probabilidad conjunta de los datos es $p(\boldsymbol{y}\vert \boldsymbol{X}; \boldsymbol{\theta})$
\item $p(\boldsymbol{y}\vert \boldsymbol{X}; \boldsymbol{\theta})$ se interpreta como función de los datos para $\boldsymbol{\theta}$ constante

\item Cuando queremos interpretar a $p(\boldsymbol{y}\vert \boldsymbol{X}; \boldsymbol{\theta})$ como función de $\boldsymbol{\theta}$ la llamamos verosimilitud (likelihood)

\begin{equation*}
L(\boldsymbol{\theta})= L(\boldsymbol{\theta}; \boldsymbol{X},\boldsymbol{y})= p(\boldsymbol{y}\vert \boldsymbol{X};\boldsymbol{\theta})
\end{equation*}

\item Si suponemos que $\epsilon^{(i)}$ son i.i.d, entonces 
 \begin{equation*}
 L(\boldsymbol{\theta})=\prod_{i=1}^{m}{ p(\boldsymbol{y}^{(i)}\vert \boldsymbol{x}^{(i)};\boldsymbol{\theta})}= \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi \sigma}} exp \left( -\frac{(y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)})^2}{2\sigma^2} \right) 
 \end{equation*}

\end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Verosimilitud logarítmica}

\begin{itemize}
\item La verosimilitud logarítmica (log likelihood) es 

\begin{equation*} \label{eq1}
\begin{split}
l(\boldsymbol{\theta}) & = ln(L(\boldsymbol{\theta}))  \\
& = ln \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi \sigma}} exp \left( -\frac{(y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)})^2}{2\sigma^2} \right)  \\
& = \sum{i=1}^{m} ln \frac{1}{\sqrt{2\pi \sigma}} exp \left( -\frac{(y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)})^2}{2\sigma^2} \right)\\
& = m ln \frac{1}{{2\pi \sigma}} - \frac{1}{\sigma^{2}}\frac{1}{2} \sum_{i}^{m} (y^{(i)}-{\boldsymbol{\theta} ^T}\boldsymbol{x}^{(i)})^2 
\end{split}
\end{equation*}



\item Observe que maximizar $l(\boldsymbol{\theta})$ eslo mismo que minimizar $J(\boldsymbol{\theta})$ 
\item Con suposiciones probabilíticas: regresión de mínimos cuadrados es equivalente a estimación de máxima verosimilitud de $\boldsymbol{\theta}$ (Note irrelevancia $\sigma$)
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Selección de características}

\begin{itemize}
\item En ejemplo de precios de casas, tenemos varias características (features) a disposición: 
\begin{itemize}
\item Área habitable 
\item Número de pisos 
\item Número de habitaciones 
\end{itemize}

\item Selección de las cuáles características usar es un criterio de diseños 
\item Es posible introducir características artificiales para introducir no linealidad en un proceso en principio lineal:
\begin{equation*}
\boldsymbol{x}= [1 \quad x_{1} \quad x_{1}^{2} \cdots x_{1}^{n}]
\end{equation*}
lo que permite modelar aproximaciones de Taylor de n-ésimo orden, de cualquier función no lineal.

\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Selección de características}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Selección de características}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Tables}
  \begin{table}
    \caption{Largest cities in the world (source: Wikipedia)}
    \begin{tabular}{lr}
      \toprule
      City & Population\\
      \midrule
      Mexico City & 20,116,842\\
      Shanghai & 19,210,000\\
      Peking & 15,796,450\\
      Istanbul & 14,160,467\\
      \bottomrule
    \end{tabular}
  \end{table}
\end{frame}
\begin{frame}{Blocks}
  Three different block environments are pre-defined and may be styled with an
  optional background color.

  \begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
      \begin{block}{Default}
        Block content.
      \end{block}

      \begin{alertblock}{Alert}
        Block content.
      \end{alertblock}

      \begin{exampleblock}{Example}
        Block content.
      \end{exampleblock}

    \column{0.5\textwidth}

      \metroset{block=fill}

      \begin{block}{Default}
        Block content.
      \end{block}

      \begin{alertblock}{Alert}
        Block content.
      \end{alertblock}

      \begin{exampleblock}{Example}
        Block content.
      \end{exampleblock}

  \end{columns}
\end{frame}
\begin{frame}{Math}
  \begin{equation*}
    e = \lim_{n\to \infty} \left(1 + \frac{1}{n}\right)^n
  \end{equation*}
\end{frame}
\begin{frame}{Line plots}
  \begin{figure}
    \begin{tikzpicture}
      \begin{axis}[
        mlineplot,
        width=0.9\textwidth,
        height=6cm,
      ]

        \addplot {sin(deg(x))};
        \addplot+[samples=100] {sin(deg(2*x))};

      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}
\begin{frame}{Bar charts}
  \begin{figure}
    \begin{tikzpicture}
      \begin{axis}[
        mbarplot,
        xlabel={Foo},
        ylabel={Bar},
        width=0.9\textwidth,
        height=6cm,
      ]

      \addplot plot coordinates {(1, 20) (2, 25) (3, 22.4) (4, 12.4)};
      \addplot plot coordinates {(1, 18) (2, 24) (3, 23.5) (4, 13.2)};
      \addplot plot coordinates {(1, 10) (2, 19) (3, 25) (4, 15.2)};

      \legend{lorem, ipsum, dolor}

      \end{axis}
    \end{tikzpicture}
  \end{figure}
\end{frame}
\begin{frame}{Quotes}
  \begin{quote}
    Veni, Vidi, Vici
  \end{quote}
\end{frame}

{%
\setbeamertemplate{frame footer}{My custom footer}
\begin{frame}[fragile]{Frame footer}
    \themename defines a custom beamer template to add a text to the footer. It can be set via
    \begin{verbatim}\setbeamertemplate{frame footer}{My custom footer}\end{verbatim}
\end{frame}
}

\begin{frame}{References}
  Some references to showcase [allowframebreaks] \cite{knuth92,ConcreteMath,Simpson,Er01,greenwade93}
\end{frame}

\section{Conclusion}

\begin{frame}{Summary}

  Get the source of this theme and the demo presentation from

  \begin{center}\url{github.com/matze/mtheme}\end{center}

  The theme \emph{itself} is licensed under a
  \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons
  Attribution-ShareAlike 4.0 International License}.

  \begin{center}\ccbysa\end{center}

\end{frame}

{\setbeamercolor{palette primary}{fg=black, bg=yellow}
\begin{frame}[standout]
  Questions?
\end{frame}
}

\appendix

\begin{frame}[fragile]{Backup slides}
  Sometimes, it is useful to add slides at the end of your presentation to
  refer to during audience questions.

  The best way to do this is to include the \verb|appendixnumberbeamer|
  package in your preamble and call \verb|\appendix| before your backup slides.

  \themename will automatically turn off slide numbering and progress bars for
  slides in the appendix.
\end{frame}

\begin{frame}[allowframebreaks]{References}

  \bibliography{demo}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}
