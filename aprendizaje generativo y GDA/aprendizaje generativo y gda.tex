\documentclass{beamer}
\usetheme{ALUF}

\usepackage[utf8]{inputenc}
% \usepackage{palatino}
% \usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[expert]{mathdesign}
\usepackage[protrusion=true,expansion=true,tracking=true,kerning=true]{microtype}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
%% Use any fonts you like.
% \usepackage{libertine}

\title{Aprendizaje generativo}
\subtitle{Reconocimiento de patrones}
\author{Gamaliel Moreno}
\date{Enero-Julio 2021}
%\institute{\url{gamalielmch@uaz.edu.mx}}

\institute{\url{gamalielmch@uaz.edu.mx}\\\url{http://pds.uaz.edu.mx/}}

\begin{document}

\begin{frame}[plain,t]
\titlepage
\end{frame}

\begin{frame}% [plain,t]
	\frametitle{Contenido}
\tableofcontents
\end{frame}

%=============================================================================================

\section{Introducción}
\subsection{Aprendizajes discriminador y generativo}
\begin{frame}
\frametitle{Aprendizaje discriminador}
 \begin{itemize}
 \item Hasta ahora, aprendizaje basado en $p(y\vert \boldsymbol{x};\boldsymbol{\theta})$ 
 \begin{itemize}
 \item Regresión logística: $p(y\vert \boldsymbol{x};\boldsymbol{\theta})= h_{\boldsymbol{\theta}}(\boldsymbol{x})= g(\boldsymbol{\theta}^T \boldsymbol{x})$ con $g(\cdot)$ sigmoidal.
 \end{itemize}  
\item Concepto actual ha particionado el espacio de características con un borde de decisión
\item Clasificación se reduce a evaluar en qué lado del borde de decisión está la entrada
\item Algortimos que aprenden $p(y\vert x)$ directamente se llaman algoritmos discriminadores
\item Pueden aprender $h_{\boldsymbol{\theta}}(\boldsymbol{x}) \in \left\lbrace 0,1 \right\rbrace $
 
 \end{itemize}

\end{frame}
%%%=============================================================================================
\section{Métodos generativos}
\subsection{Análisis Guassiano discriminador}
\begin{frame}
\frametitle{Aprendizaje generativo}

\begin{itemize}
\item Otra idea: aprender $p(\boldsymbol{x}\vert y)$ y $p(y)$
\item Ejemplo: aprendamos con características de forma/textura modelos para 
\begin{itemize}
\item Cáncer benigno 
\item Cáncer maligno
\end{itemize}
\item Para cada clase aprendemos un modelo por separado
\item Para predicción, deben probarse todos los modelos y se selecciona el más probable
\item Este enfoque se denomina aprendizaje generativo. 
\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Análisis Gaussiano discriminador}

\begin{itemize}

\item Supongamos que $\boldsymbol{x} \in \mathbb{R}^n$ son continuos
\item Además supongamos que $p(\boldsymbol{x}\vert y)$ es guassiano

\begin{equation*}
p(\boldsymbol{x}\vert y)= \mathbf{N} (\mu,\Sigma) 
\end{equation*}

donde $\mu$ es la media y $\Sigma$ es la matriz de covarianza 
\end{itemize}

\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Análisis Gaussiano discriminador}

\begin{itemize}
\item Supongamos que 
\begin{equation*}
\begin{split}
p(y) &=\phi^y(1-\phi)^{1-y}\\
p(\boldsymbol{x}\vert y=0) &= \frac{1}{\sqrt{(2\pi)^n \vert  \boldsymbol{\Sigma} \vert }} exp \left( -\frac{1}{2} (\boldsymbol{x}-\mu_0)^T  \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\mu_0) \right)  \\
p(\boldsymbol{x}\vert y=1) &= \frac{1}{\sqrt{(2\pi)^n \vert  \boldsymbol{\Sigma} \vert }} exp \left( -\frac{1}{2} (\boldsymbol{x}-\mu_1)^T  \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\mu_1) \right)
\end{split}
\end{equation*}

\item Buscamos entonces maximizar la verosimilitud 

\begin{equation*}
\ell (\phi, \mu_0, \mu_1, \boldsymbol{\Sigma})= ln  \underbrace{\prod_{i}^{m}{p(\boldsymbol{x}^{(i)},y^{(i)})}}_\text{verosimilitud conjunta} = \prod_{i}^{m}{p(\boldsymbol{x}^{(i)}\vert y^{(i)}) p(y^{(i)})}
\end{equation*}
\end{itemize}


\end{frame}
%%
%=============================================================================================
\begin{frame}
\frametitle{Análisis Gaussiano discriminador}
\begin{itemize}
\item Esta verosimilitud conjunta 
\begin{equation*}
\ell (\phi, \mu_0, \mu_1, \boldsymbol{\Sigma})= \prod_{i}^{m}{p(\boldsymbol{x}^{(i)}\vert y^{(i)}) p(y^{(i)})}
\end{equation*}
contrasta con la verosimilitud condicional utilizada en regresión logística

\begin{equation*}
\ell (\boldsymbol{\theta})= ln \prod_{i}^{m} {p(y^{(i)} \vert \boldsymbol{x}^{(i)}; \boldsymbol{\theta})} 
\end{equation*}

\end{itemize}


\end{frame}
%
%=============================================================================================
\begin{frame}
\frametitle{Análisis Gaussiano discriminador}

\begin{itemize}
\item Maximizando la verosimilitud anterior se obtiene
\begin{equation*}
\phi= \frac{1}{m} \sum_{i=1}^{m} 1\lbrace y^{(i)}=1\rbrace 
\end{equation*}

\begin{equation*}
\boldsymbol{\mu}_0 = \frac{\sum_{i=1}^{m} 1\lbrace y^{(i)}=0\rbrace \boldsymbol{x}^{(i)}}{\sum_{i=1}^{m} 1\lbrace y^{(i)}=0\rbrace}
\end{equation*}

\begin{equation*}
\boldsymbol{\mu}_1 = \frac{\sum_{i=1}^{m} 1\lbrace y^{(i)}=1\rbrace \boldsymbol{x}^{(i)}}{\sum_{i=1}^{m} 1\lbrace y^{(i)}=1\rbrace}
\end{equation*}
\begin{equation*}
\boldsymbol{\Sigma}= \frac{1}{m}  \sum_{i=1}^{m}(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{y^{(i)}} )(\boldsymbol{x}^{(i)}-\boldsymbol{\mu}_{y^{(i)}} )^T
\end{equation*}

\end{itemize}


\end{frame}
%
%=============================================================================================

\begin{frame}
\frametitle{Predicción}

\begin{itemize}
\item Observemos que con la regla de Bayes, puede recalcularse:
\begin{equation*}
p(y=1\vert \boldsymbol{x})= \frac{p(\boldsymbol{x}\vert y=1)p(y)}{p(\boldsymbol{x})}
\end{equation*}   
\begin{equation*}
p(\boldsymbol{x})= p(\boldsymbol{x}\vert y=0)p(y=0)+ p(\boldsymbol{x}\vert y=1)p(y=1) 
\end{equation*}
\item sin embargo, $p(\boldsymbol{x})$ usualmente es innecesario pues para la predicción basta con:
\begin{equation*}
\arg\max_{y} p(y \vert \boldsymbol{x})=  \arg\max_{y} \frac{p(\boldsymbol{x} \vert y) p(y)}{p(\boldsymbol{x})}= \arg\max_{y} p(\boldsymbol{x} \vert y) p(y)
\end{equation*}
\item Si $p(y)$ es uniforme ($p(y=0)=p(y=1)$) entonces: $\arg\max_{y} p(\boldsymbol{x} \vert y)$
\end{itemize}
\end{frame}


%%%=============================================================================================

\begin{frame}
\frametitle{GDA}
\begin{itemize}
\item Dado el conjunto de entrenamiento $(\boldsymbol{x}^{(i)}, y^{(i)})$ 
\item Calcular con el conjunto los parámetros $\boldsymbol{\mu}_i, \Sigma$ y $p(y)$
\item Para predecir probabilidad de $y$ dado un valor de $\boldsymbol{x}$: 
\begin{itemize}
\item Calculamos con parámetros $p(\boldsymbol{x} \vert y=0)= \mathbf{N}(\boldsymbol{\mu}_0,\sigma^2_0)$ y $p(\boldsymbol{x} \vert y=1)= \mathbf{N}(\boldsymbol{\mu}_1,\sigma^2_1)$
\item con eso calculamos 
\begin{equation*}
p(y=1\vert \boldsymbol{x})= \frac{p(\boldsymbol{x}\vert y=1)p(y)}{p(\boldsymbol{x})}
\end{equation*}
donde  $p(\boldsymbol{x})$ se calcula con 
\begin{equation*}
p(\boldsymbol{x})= p(\boldsymbol{x}\vert y=0)p(y=0)+ p(\boldsymbol{x}\vert y=1)p(y=1) 
\end{equation*}

\end{itemize}
\end{itemize}
\end{frame}
%%%=============================================================================================

\begin{frame}
\frametitle{Ventajas y desventajes de algoritmos generativos }
 \begin{itemize}
 \item En GDA supusimos $\boldsymbol{x}\vert y \backsim$ gaussiano
 \item Eso implica que la distribución a-posteriori $p(y=1 \vert \boldsymbol{x})$ es logística 
 \item Lo contario no es cierto: logístico $\nrightarrow \boldsymbol{x} \vert y \backsim$  gaussiano
 \item (por ejemplo, si $\boldsymbol{x} \vert y \backsim$ Poisson también la probabilidad a-posteriori es logística)
 \item Eso implica que suposición del GDA es más fuerte
 \item Si la suposición es cierta, entonces GDA es mejor que la regresión logística 
 \item Si no se sabe qué distribución tiene los datos, entonces la regresión logística es una mejor elección 
 \item GDA funciona a veces mejor con pocos datos
 \item Regresión logística requiere por lo general más datos
 \end{itemize}
\end{frame}
%%%=============================================================================================

\subsection{Clasificador Bayesiano ingenuo}
\begin{frame}
\frametitle{Clasificador bayesiano ingenuo}
\begin{center}
Clasificador bayesiano ingenuo


(Segundo método generativo)
\end{center}

\end{frame}
%%%=============================================================================================

%=============================================================================================
\begin{frame}
\frametitle{Características}


\begin{itemize}
\item Representación usa un vector de dimensión igual al número de palabras en el diccionario  
\item Si el correo-e contiene la iésima palabra del diccionario usamos $x_{i}=1$, y caso contrario $x_{i}=0$
\item Por ejemplo 
\begin{equation*}
x=\left(\begin{array}{c} 
1\\ 
0\\
0\\
\vdots \\
1 \\
\vdots \\
0
\end{array}\right)\begin{array}{c}
	a \\
	ababa \\
	ababillarse \\
	\vdots \\ 
	compra  \\
	\vdots \\
	zwingliano
\end{array}
\end{equation*}
\end{itemize}
\end{frame}
%%%=============================================================================================

\begin{frame}
\frametitle{Vocabulario}


\begin{itemize}
\item Conjunto de palabra codificada en el vector de características se llama vocabulario
\item Tamaño del vocabulario igual a dimensión de $\boldsymbol{x}$
\item Si tenemos un vocabulario de 50,000 palabras, entonces $\boldsymbol{x} \in \lbrace 0; 1\rbrace$
\item Queremos armar un modelo generativo, así que necesitamos un modelo para $p(\boldsymbol{x}\vert y)$
\item Obviamente no es posible modelar cada $\boldsymbol{x}$ explícitamente con un modelo multinomial, pues tendríamos $2^{50000}$ posibles configuraciones, lo que implica un vector de configuración de $(2^{50000} -1)$ dimensiones  

\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Probabilidad conjunta condicional}
\begin{equation*}
\begin{split}
p(\boldsymbol{x} \vert y)&= p(x_{1}, \ldots, x_{50,000} \vert)\\
&=  p(x_{1} \vert y) p(x_{2} \vert y, x_{1}) \cdot p(x_{50,000} \vert y, x_{1}, \ldots x_{49,999})\\
&= p(x_{1} \vert y) p(x_{2} \vert y )\cdots  p(x_{50,000} \vert y )\\
&= \prod_{i=1}^{n} p(x_i\vert y)
\end{split}
\end{equation*}

\begin{itemize}
\item A pesar de que esta suposición es muy fuerte, el método funciona. 
\item El modelo se parametriza con $\phi_{i\vert y=1}=p(x_{i}=1\vert y=1 )$, $ \phi_{i\vert y=0}=p(x_{i}=1\vert y=0 )$ y $ \phi_{y}=p(y=1 )$

\end{itemize}


\end{frame}
%
%%%=============================================================================================
\begin{frame}
\frametitle{Máxima verosimilitud}

\begin{itemize}
\item Si se maximiza $L(\phi_{y},\phi_{j\vert y=0}, \phi_{j\vert y=1})$ con respecto a los parámetros, se obtiene el estimado de máxima verosimilitud:
\begin{equation*}
\phi_{j\vert y=1}=p(x_{j}=1\vert y=1 )= \frac{\sum_i^m 1 \lbrace x_j^{(i)} \wedge y^{(i)}=1 \rbrace  }{\sum_i^m  \lbrace  y^{(i)}=1 \rbrace }
\end{equation*}
\begin{equation*}
\phi_{j\vert y=0}=p(x_{j}=1\vert y=0 )= \frac{\sum_i^m  1 \lbrace x_j^{(i)} \wedge y^{(i)}=0 \rbrace  }{\sum_i^m  \lbrace  y^{(i)}=0 \rbrace }
\end{equation*}
\begin{equation*}
\phi_{y}=p(y=1 )= \frac{\sum_i^m  1 \lbrace y^{(i)}=1 \rbrace  }{m}
\end{equation*}

\item Interpretaciones fáciles 

\end{itemize}

\end{frame}%=============================================================================================

\begin{frame}
\frametitle{Máxima verosimilitud}

\begin{itemize}
\item Con estos parámetros, para hacer la predicción en un nuevo correo $ \boldsymbol{x}$ solo calculamos: 

\begin{equation*}
p(y=1\vert \boldsymbol{x})=  \frac{p(\boldsymbol{x}\vert y=1) p(y=1)}{p(c)}
\end{equation*}

\begin{equation*}
=\frac{\left(  \prod_{i=1}^{n} p(x_i\vert y=1) \right)p(y=1) }{\left(  \prod_{i=1}^{n} p(x_i\vert y=1) \right) p(y=1) + \left( \prod_{i=1}^{n} p(x_i\vert y=0) \right) p(y=0) }
\end{equation*}

\item Elegimos la clase que tenga la probabilidad a-posteriori mayor

\end{itemize}
\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Caso multinomial}
\begin{itemize}
\item Desarrollamos el algoritmo de Bayes ingenuo para características de entrada $x_{i}$ binarias
\item Nada impide usar características $x_{i}\in \lbrace1, 2, \ldots, k_{i}\rbrace$
\item En ese caso modelamos $p(x_{i}\vert y)$ con una distribución multinomial en vez de Bernoulli
\item En la práctica, en problemas con entras continuas, se obtienen buenos resultados se se discretiza  la entrada y se usa el algoritmo de Bayes ingenuo (por ejemplo, si datos no siguen una dsitribución normal multivariada)
\end{itemize}

\end{frame}
%%%=============================================================================================
\begin{frame}
\frametitle{Suavizamiento con laplace}
\begin{center}
Suavizamiento con Laplace
\end{center}


\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Suavizamiento con laplace}
\begin{itemize}
\item El algoritmo ingenuo de Bayes funciona en bastantes problemas
\item Un cambio simple lo mejora, especialmente para clasificación textual
\item Una nueva palabra $k$ que no estuvo en el conjunto de entrenamiento tendrá:
\end{itemize}
\begin{equation*}
\phi_{k\vert y=1}= \frac{\sum_i^m 1 \lbrace x_j^{(i)} \wedge y^{(i)}=1 \rbrace  }{\sum_i^m  \lbrace  y^{(i)}=1 \rbrace }=0
\end{equation*}
\begin{equation*}
\phi_{k\vert y=0}= \frac{\sum_i^m  1 \lbrace x_j^{(i)} \wedge y^{(i)}=0 \rbrace  }{\sum_i^m  \lbrace  y^{(i)}=0 \rbrace }=0
\end{equation*}

\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Problema con suposición ingenua}
\begin{itemize}
\item Como la plabra no es spam no no-spam, la probabilidad de que cualquiera ocurra es cero
\item Si queremos decidir qué tipo de correo es uno que contenga la k-ésima palabra se obtiene: 

\begin{equation*}
\begin{split}
&\frac{\left(  \prod_{i=1}^{n} p(x_i\vert y=1) \right)p(y=1) }{\left(  \prod_{i=1}^{n} p(x_i\vert y=1) \right) p(y=1) + \left( \prod_{i=1}^{n} p(x_i\vert y=0) \right) p(y=0) } \\
&= \frac{0}{0}
\end{split}
\end{equation*}  
\end{itemize}

\end{frame}
%=============================================================================================
\begin{frame}
\frametitle{Suavizamiento de Laplace}
\begin{itemize}
\item Estadísticamente es mala idea suponer que la probabilidad de un evento es cero solo porque no se ha visto en el conjunto de entrenamiento
\item Para $m$ observaciones, estimación de máxima verosimilitud es: 
\begin{equation*}
\phi_j=\frac{\sum_{i=1}^m 1 \lbrace x^{i}=j \rbrace}{m}
\end{equation*}

\item Con esta estimación algunos $phi_j$ pueden llegar a ser cero, lo que se evita con el suavizamiento de Laplace, que lo reemplaza con 
\begin{equation*}
\phi_j=\frac{\sum_{i=1}^m 1 \lbrace x^{i}=j \rbrace+1}{m+k}
\end{equation*}

\item $k$ es el número de posibles valores que puede tomar $x^{(i)}$ (en el caso binario k=2)
\item Note que aún se cumple $\sum_{j=1}^k \phi_j=1$ y $\phi_j\neq 0$
\end{itemize}



\end{frame}

%=============================================================================================
%=============================================================================================
\end{document}
